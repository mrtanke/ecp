{"cells":[{"cell_type":"markdown","id":"6e96b120-4cca-4089-ac37-09661299e1b1","metadata":{"id":"6e96b120-4cca-4089-ac37-09661299e1b1"},"source":["#  **Lab 03: Implementing a Manual Quantized CNN with Look-Up Table (LUT)-Based Multiplication**\n","\n","## ğŸ” **Introduction**\n","In this lab, we will explore how to implement a **manual Convolutional Neural Network (CNN)** from scratch and replace its built-in multiplication operations with a **Look-Up Table (LUT)-based multiplication** mechanism.  \n","\n","In typical CNN implementations, each convolution or dense layer performs thousands of multiplications between input activations and weights.  \n","When implementing **approximate multipliers**, repeatedly calling a custom function for each multiplication would be very slow.  \n","A much more efficient approach is to use a **Look-Up Table (LUT)** that stores the precomputed results of all possible multiplication pairs.  \n","Then, during inference, the model can simply read results from the table instead of performing direct arithmetic â€” a concept that significantly reduces computation time.\n","\n","In this lab, to simplify the concept and keep the focus on the **LUT mechanism itself**, we will **use the LUT of an exact multiplier** (i.e., the results are mathematically correct).  \n","This allows you to understand how LUT-based computation works before moving to **approximate multipliers** in your homework.\n","\n","---\n","\n","## ğŸ¯ **Objectives**\n","By the end of this lab, you will be able to:\n","\n","1. **Implement** a manual 2D convolution function using LUT-based (exact) multiplication.  \n","2. **Modify** dense (fully connected) layers to use LUT-based multiplication as well.  \n","3. **Construct** a quantized *LeNet5-Inspired CNN* entirely from scratch using your custom convolution and dense functions.  \n","4. **Evaluate** the CNNâ€™s performance and accuracy when all multiplications are replaced with LUT lookups.\n","\n","---\n","\n","> ğŸ§  **Key Concept:**  \n","> Instead of computing `a Ã— b` directly, we use a **Look-Up Table** to fetch the result of that multiplication.  \n","> This approach forms the foundation for implementing **approximate arithmetic**, which you will explore in your homework.\n"]},{"cell_type":"markdown","id":"e1c68a71-ecdb-4b7f-bc79-2e85efa1f068","metadata":{"id":"e1c68a71-ecdb-4b7f-bc79-2e85efa1f068"},"source":["## ğŸ§© **Step 1: Import Required Libraries**"]},{"cell_type":"code","execution_count":1,"id":"516b079a-bd74-4a7e-b186-b25f8cf9beae","metadata":{"id":"516b079a-bd74-4a7e-b186-b25f8cf9beae","executionInfo":{"status":"ok","timestamp":1761732423115,"user_tz":-60,"elapsed":6024,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, Model\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","from tqdm import tqdm"]},{"cell_type":"markdown","id":"f45ad6a0-dc65-4d33-aa90-814d9d0ed06a","metadata":{"id":"f45ad6a0-dc65-4d33-aa90-814d9d0ed06a"},"source":["## ğŸ§© **Step 2: Load the MNIST Dataset**"]},{"cell_type":"code","execution_count":2,"id":"9e090b1e-2155-4e07-8293-bbc3738e8e2b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"9e090b1e-2155-4e07-8293-bbc3738e8e2b","executionInfo":{"status":"ok","timestamp":1761732423943,"user_tz":-60,"elapsed":823,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}},"outputId":"2c3e2026-462d-4783-a944-f9e583ea75e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x300 with 5 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training data shape: (60000, 28, 28, 1) (60000,)\n","Test data shape: (10000, 28, 28, 1) (10000,)\n"]}],"source":["# -------------------------\n","# 1) Data: load + preprocess\n","# -------------------------\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","# MNIST is 28x28 grayscale.\n","x_train = x_train.astype(\"uint8\")\n","x_test  = x_test.astype(\"uint8\")\n","x_train = x_train.reshape(-1, 28, 28, 1)\n","x_test  = x_test.reshape(-1, 28, 28, 1)\n","\n","x_test_fp = x_test\n","\n","#To visualize three test images from the testing set along with their corresponding labels, you can use the following code:\n","# Define the number of images to display\n","num_images = 5\n","\n","# Create subplots for each image\n","plt.figure(figsize=(10, 3))\n","for i in range(num_images):\n","    plt.subplot(1, num_images, i + 1)\n","    plt.imshow(x_test[i], cmap='gray')\n","    plt.title(f\"Label: {y_test[i]}\")\n","    plt.axis('off')  # Hide the axis\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Check the shape of the data\n","print(\"Training data shape:\", x_train.shape, y_train.shape)\n","print(\"Test data shape:\", x_test.shape, y_test.shape)\n"]},{"cell_type":"markdown","id":"efbd8612-d76e-43ab-9c6f-43465d6d907b","metadata":{"id":"efbd8612-d76e-43ab-9c6f-43465d6d907b"},"source":["## ğŸ§© **Step 3: Applying 8-bit Quantization to the MNIST Dataset**"]},{"cell_type":"code","execution_count":3,"id":"2939d850-271a-4fc6-ba02-e1bea1eeebd2","metadata":{"id":"2939d850-271a-4fc6-ba02-e1bea1eeebd2","executionInfo":{"status":"ok","timestamp":1761732424410,"user_tz":-60,"elapsed":464,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["# 1. Load MNIST dataset\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","# 2ï¸. Convert to float32 for safe division\n","x_test = tf.cast(x_test, tf.float32)\n","\n","# 3ï¸. Normalize pixel values from [0, 255] â†’ [0, 1]\n","x_test = x_test / 255.0\n","\n","# 4ï¸. Scale normalized values from [0, 1] â†’ [0, 127]\n","x_test = x_test * 127.0\n","\n","# 5ï¸. Round to nearest integer\n","x_test = tf.round(x_test)\n","\n","# 6ï¸. Clip to make sure everything is within [0, 127]\n","x_test = tf.clip_by_value(x_test, 0.0, 127.0)\n","\n","# 7. Reshape to (num_samples, 28, 28, 1)\n","x_test = tf.reshape(x_test, (-1, 28, 28, 1))\n"]},{"cell_type":"markdown","id":"a0d6b21a-7164-4b34-9fad-fdcc4a050404","metadata":{"id":"a0d6b21a-7164-4b34-9fad-fdcc4a050404"},"source":["## ğŸ§© **Step 4: Load the Pre-trained LeNet-5 Inspired Model with Quantization**\n","\n","In this step, we will load a **pre-trained and quantized LeNet-5-Inspired model**.  \n","This model has already been trained on the MNIST dataset to recognize handwritten digits and is designed to operate using **quantized weights and activations**.  \n","By using this pre-trained network, we can focus on understanding **how inference works** and how to integrate **LUT-based arithmetic**.\n","\n","---\n","\n","### ğŸ§  **What is Quantization?**\n","\n","Quantization is the process of converting the **32-bit floating-point** values used in neural networks (for weights, biases, and activations) into **lower-precision integer values** â€” often 8-bit integers. This helps to make the network smaller, faster, and more efficient for deployment on edge devices or hardware accelerators.\n","\n","---\n","\n","### âš–ï¸ **Advantages and Trade-offs of Quantization**\n","\n","#### âœ… *Advantages:*\n","- **Reduced Model Size:** Lower-bit weights take up less memory, which is crucial for embedded devices.  \n","- **Faster Inference:** Integer arithmetic is much faster than floating-point arithmetic, enabling near real-time performance.  \n","- **Lower Power Usage:** Fewer computational resources mean reduced power consumption â€” ideal for battery-powered systems.\n","\n","#### âš ï¸ *Potential Drawbacks:*\n","- **Slight Accuracy Loss:** Converting from float to int can introduce rounding errors.  \n","- **Limited Precision:** Very small weight updates or activation variations may be lost due to the reduced numerical range.\n","\n","---\n","> We will later apply our **LUT-based multiplications** to this quantized model to simulate integer arithmetic behavior.\n"]},{"cell_type":"code","execution_count":4,"id":"9d6b9efa-aca2-434c-b543-3ebc270a6af8","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":848},"id":"9d6b9efa-aca2-434c-b543-3ebc270a6af8","executionInfo":{"status":"ok","timestamp":1761732424793,"user_tz":-60,"elapsed":378,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}},"outputId":"0ffff921-5337-4a7a-e00e-9eb3974b6896"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_8 (\u001b[38;5;33mActivation\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_9 (\u001b[38;5;33mActivation\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     â”‚           \u001b[38;5;34m528\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_10 (\u001b[38;5;33mActivation\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m8\u001b[0m)      â”‚         \u001b[38;5;34m1,160\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_11 (\u001b[38;5;33mActivation\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m8\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m8\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m4\u001b[0m)      â”‚           \u001b[38;5;34m292\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_12 (\u001b[38;5;33mActivation\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m4\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m4\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m295,040\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_13 (\u001b[38;5;33mActivation\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_14 (\u001b[38;5;33mActivation\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚           \u001b[38;5;34m650\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_15 (\u001b[38;5;33mActivation\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,160</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ activation_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m308,134\u001b[0m (1.18 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">308,134</span> (1.18 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m308,134\u001b[0m (1.18 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">308,134</span> (1.18 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["# Load your trained FP32 model\n","quantized_model = load_model(\"my_org_model_top4_quant.h5\", compile=False)\n","quantized_model.summary()"]},{"cell_type":"markdown","id":"58e24ba5-6a76-47ab-97d7-7452521d8209","metadata":{"id":"58e24ba5-6a76-47ab-97d7-7452521d8209"},"source":["## ğŸ§© **Step 5: Overview and Analysis of Our CNN Model Architecture**\n","\n","In this step, we will explore the structure of our **custom CNN model**, which is inspired by the classic **LeNet-5 architecture** â€” one of the first successful convolutional neural networks for image classification. This model is designed to recognize handwritten digits (0â€“9) from the MNIST dataset.  \n","We will also retrieve and examine the **weights** and **biases** of each layer to better understand how the model processes input data and learns visual patterns.\n","\n","---\n","\n","### ğŸ—ï¸ **Model Structure**\n","\n","Our CNN consists of three main components:\n","\n","#### ğŸ”¹ **Convolutional Layers**\n","- The model begins with **five convolutional layers**.  \n","  Each layer applies a set of **filters (kernels)** that slide across the input image to detect specific local patterns such as edges, corners, and textures.  \n","- After each convolution, an **activation function (ReLU)** introduces non-linearity, allowing the network to learn more complex representations of the data.\n","\n","#### ğŸ”¹ **Flattening Layer**\n","- Once the convolutional feature extraction is complete, the resulting 2D feature maps are **flattened** into a 1D array.  \n","  This operation prepares the extracted features for the **fully connected layers**, which handle the final classification.\n","\n","#### ğŸ”¹ **Dense (Fully Connected) Layers**\n","- After flattening, the features are passed through **three dense layers**.  \n","  The first two dense layers are followed by activation functions that help the network learn higher-level feature combinations.  \n","  The final dense layer outputs **10 values**, each representing the probability that the input image belongs to one of the 10 digit classes (0â€“9).\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":5,"id":"1fbafff4-dcc8-4786-ba48-06d0ee7f07ca","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1fbafff4-dcc8-4786-ba48-06d0ee7f07ca","executionInfo":{"status":"ok","timestamp":1761732424796,"user_tz":-60,"elapsed":2,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}},"outputId":"4a0b6643-96ff-457a-e4ca-3f742064c4c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------\n","Convolution Layers:\n","----------------------------------------\n","Layer: Conv2D\n","  Filters: shape = (1, 1, 1, 64)\n","  Biases: shape = (64,)\n","Layer: Conv2D\n","  Filters: shape = (1, 1, 64, 32)\n","  Biases: shape = (32,)\n","Layer: Conv2D\n","  Filters: shape = (1, 1, 32, 16)\n","  Biases: shape = (16,)\n","Layer: Conv2D\n","  Filters: shape = (3, 3, 16, 8)\n","  Biases: shape = (8,)\n","Layer: Conv2D\n","  Filters: shape = (3, 3, 8, 4)\n","  Biases: shape = (4,)\n","----------------------------------------\n","Dense Layers (fully-connected Layers):\n","----------------------------------------\n","Dense_Layer 1 has 128 neurons\n","Dense_Layer 2 has 64 neurons\n","Dense_Layer 3 has 10 neurons\n"]}],"source":["# Get model weights and print model summary\n","model_weights = quantized_model.get_weights()\n","\n","print(\"-\"*40)\n","print(\"Convolution Layers:\")\n","print(\"-\"*40)\n","for layer in quantized_model.layers:\n","    if 'conv' in layer.name:  # This filters only convolutional layers\n","        print(f\"Layer: Conv2D\")\n","\n","        # Get the weights and biases of the layer\n","        filters, biases = layer.get_weights()\n","\n","        print(f\"  Filters: shape = {filters.shape}\")\n","        print(f\"  Biases: shape = {biases.shape}\")\n","i =0\n","print(\"-\"*40)\n","print(\"Dense Layers (fully-connected Layers):\")\n","print(\"-\"*40)\n","for layer in quantized_model.layers:\n","    if 'dense' in layer.name:  # This filters only dense layers\n","        i += 1\n","        print(f\"Dense_Layer {i} has {layer.units} neurons\")"]},{"cell_type":"markdown","id":"6b38969d-eace-4bd5-8d35-c59dc2f74783","metadata":{"id":"6b38969d-eace-4bd5-8d35-c59dc2f74783"},"source":["## ğŸ§© **Step 6: Creating a Look-Up Table (LUT) for Multiplication**\n","\n","In this step, we will construct a **Look-Up Table (LUT)** that replaces traditional multiplication operations with **table lookups**.  \n","Instead of performing arithmetic calculations during convolution or dense operations, our model will **retrieve precomputed multiplication results** from the LUT.\n","\n","Since our CNN uses **8-bit signed integers** for weights and activations, the LUT must cover all possible combinations of 8-bit signed values â€” from **-128 to +127**.\n","\n","---\n","\n","### ğŸ”¹ **Part 1: Building the LUT for Signed 8-Bit Multiplication**\n","\n","#### âš™ï¸ **LUT Range**\n","The LUT needs to store results for every possible pair of signed 8-bit integers.\n","\n","This creates a **256 Ã— 256 table**, where each entry at position `(i, j)` represents the product of two integers `a Ã— b`.\n","\n","#### ğŸ’¾ **Storage Concept**\n","- Each LUT entry corresponds to a **unique pair** of integers.  \n","- Once built, the LUT allows fast access â€” for example:\n","  ```python\n","`a Ã— b` = `LUT[a + 128][b + 128]`\n","\n","#### Why use `+128` for LUT indexing?\n","The `+128` offset ensures that signed integers in the range **[-128, 127]** map to valid LUT indices **[0, 255]**.  \n","Example: `-128 â†’ 0`, `0 â†’ 128`, `127 â†’ 255`.\n","\n","---\n","\n","### ğŸ”¹ **Part 2: Implementing the Manual Element-Wise Multiplication Function**\n","\n","Next, weâ€™ll define a **custom element-wise multiplication function** that uses our LUT.  \n","This function will handle **2-dimensional element-wise multiplication** of two matrices with the same size â€” a common operation in **2D convolution** layers.\n"]},{"cell_type":"code","execution_count":6,"id":"7a28e3f8-5731-44b5-9aeb-41d073453382","metadata":{"id":"7a28e3f8-5731-44b5-9aeb-41d073453382","executionInfo":{"status":"ok","timestamp":1761732424807,"user_tz":-60,"elapsed":1,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["LUT = np.zeros((256,256),dtype=np.int32)\n","for i in range(-128,128):\n","    for j in range(-128,128):\n","        LUT[i +128,j+128]= i * j"]},{"cell_type":"code","execution_count":7,"id":"1543616c-8085-4daf-8f28-51838303911d","metadata":{"id":"1543616c-8085-4daf-8f28-51838303911d","executionInfo":{"status":"ok","timestamp":1761732424809,"user_tz":-60,"elapsed":1,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["def manual_elementwise_multiplication(a, b):\n","    \"\"\"\n","    Custom element-wise multiplication function.\n","    Multiplies elements of a and b using pre-computed arrays based on type t.\n","    \"\"\"\n","    a = np.array(a)\n","    b = np.array(b)\n","    a_shape = np.shape(a)\n","    b = np.reshape(b, a_shape)\n","    result = np.zeros(a_shape)\n","\n","    for i in range(a_shape[0]):\n","        for j in range(a_shape[1]):\n","            result[i, j] = LUT[int(a[i, j]) + 128, int(b[i, j]) + 128]\n","\n","    return result"]},{"cell_type":"markdown","id":"93de073c-8b78-494c-be1e-e1f3f20dc6e9","metadata":{"id":"93de073c-8b78-494c-be1e-e1f3f20dc6e9"},"source":["## ğŸ§© **Step 7: Implementing a Manual 1-Channel 2D Convolution Function with LUT-Based Multiplication**\n","\n","In this step, we implement a **custom 1-channel 2D convolution function** that uses the LUT for all multiplications.  \n","This function performs element-wise multiplications using the LUT instead of standard arithmetic.\n","\n","---\n","\n","### âš™ï¸ **Convolution Overview**\n","\n","A **2D convolution** slides a filter (kernel) across the input image.  \n","At each position, it multiplies the kernel elements with the corresponding input patch and sums the results to form one output value, representing a detected feature.\n","\n","---\n","\n","### ğŸ§  **Key Components**\n","\n","- **Sliding Window:**  \n","  The function extracts a patch of the input that matches the filter size for each position.\n","\n","- **LUT-Based Multiplication:**  \n","  Each element-wise multiplication between the input patch and the filter is replaced by a **lookup** from the LUT.\n","\n","- **Summation:**  \n","  All lookup results are summed to produce the final value for that output position in the feature map.\n"]},{"cell_type":"code","execution_count":8,"id":"1b9563cc-670d-4c26-a778-2b81ee491245","metadata":{"id":"1b9563cc-670d-4c26-a778-2b81ee491245","executionInfo":{"status":"ok","timestamp":1761732424810,"user_tz":-60,"elapsed":1,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["def manual_convolution(image, kernel):\n","    \"\"\"\n","    manual 2D convolution using manual_elementwise_multiplication function.\n","    \"\"\"\n","    # Determine the dimensions of image and kernel\n","    image_height, image_width = image.shape\n","    kernel_height, kernel_width = kernel.shape\n","\n","    # Determine the output dimensions based on the dimensions of the input image and kernel\n","    output_height = image_height - kernel_height + 1\n","    output_width = image_width - kernel_width + 1\n","\n","    # Initialize the output image by zero\n","    output = np.zeros((output_height, output_width))\n","\n","    # Perform convolution\n","    for i in range(output_height):\n","        for j in range(output_width):\n","            # Element-wise multiplication of the kernel and the corresponding image region\n","            region = image[i:i+kernel_height, j:j+kernel_width]\n","            output[i, j] = np.sum(manual_elementwise_multiplication(region , kernel))\n","\n","    return output"]},{"cell_type":"markdown","id":"981f4bf5-c821-4865-99d0-49a21edcf5a4","metadata":{"id":"981f4bf5-c821-4865-99d0-49a21edcf5a4"},"source":["## ğŸ§© **Step 8: Implementing the Complete 2D Convolution Operation**\n","\n","Now that we have built the LUT-based 1-channel convolution function,  \n","we can extend it to perform a **full 2D convolution** that supports multiple input channels and multiple filters (kernels), just like real CNN layers.\n","\n","---\n","\n","### âš™ï¸ **Understanding Full 2D Convolution**\n","\n","In a CNN, the convolution operation is not limited to a single channel or a single kernel.  \n","Instead, we can have:\n","- **Multiple input channels** (e.g., from color channels or previous feature maps).\n","- **Multiple kernels (filters)**, where each kernel produces one output feature map.\n","\n","Each kernel contains a separate **2D filter for each input channel**.  \n","During convolution, the process works as follows:\n","\n","1. For a given kernel:\n","   - Each input channel is convolved with its corresponding kernel slice using the **1-channel LUT-based convolution**.\n","   - The results from all channels are **summed** element-wise to produce one **output feature map**.\n","\n","2. This process is **repeated for all kernels**, meaning:\n","   - Each kernel produces one output feature map.\n","   - If there are multiple kernels, their results are **stacked** along the channel dimension, forming the final multi-channel output.\n","\n","---\n","\n","### ğŸ” **Summary of the Process**\n","1. Loop over each **kernel** (filter).  \n","2. For each kernel:\n","   - Loop over all **input channels**.  \n","   - Perform the **LUT-based convolution** between the input channel and the corresponding kernel slice.  \n","   - **Sum** all the resulting feature maps across channels.  \n","3. Stack all kernel outputs to form the final multi-channel output.\n","\n","---\n","\n","> ğŸ’¡ **Key Idea:**  \n","> A full 2D convolution is essentially a combination of many **1-channel convolutions**,  \n","> where results are **summed across channels** and **repeated across kernels** to extract rich features from the input.\n"]},{"cell_type":"code","execution_count":10,"id":"926afed0-b3f2-4aa3-b6d1-fecb58edc3b4","metadata":{"id":"926afed0-b3f2-4aa3-b6d1-fecb58edc3b4","executionInfo":{"status":"ok","timestamp":1761734608228,"user_tz":-60,"elapsed":2,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["def convolution_layer(input_feature_map, kernel, bias):\n","\n","    input_shape = input_feature_map.shape # [input_height, output_height, channels]\n","    kernel_shape = kernel.shape # [kernel_height, kernel_weight, in_channels, out_channels]\n","\n","    output_height = input_shape[0] - kernel_shape[0] + 1\n","    output_width = input_shape[1] - kernel_shape[1] + 1\n","    output_channels = kernel_shape[3]\n","\n","    output = np.zeros([output_height, output_width, output_channels])\n","\n","    for i in range(output_channels):\n","        for j in range(kernel_shape[2]):\n","            output[:, :, i] = output[:, :, i] + manual_convolution( input_feature_map[ :, :,j] , kernel[ :, :, j, i] )\n","        output[:, :, i] = output[:, :, i] + bias[i]\n","\n","    return output"]},{"cell_type":"markdown","id":"42e3b9ec-2bb6-4c88-976f-ee61328eb58c","metadata":{"id":"42e3b9ec-2bb6-4c88-976f-ee61328eb58c"},"source":["## ğŸ§© **Step 9: Implementing a Manual Dense (Fully Connected) Layer**\n","\n","In this step, we implement a **manual dense layer** that performs matrix multiplication using our **LUT-based multiplication** functions.\n","\n","---\n","\n","### âš™ï¸ **How It Works**\n","\n","1. **`manual_elementwise_1D_multiplication(a, b)`**  \n","   - Performs element-wise multiplication between two 1D vectors using the **LUT** instead of the `*` operator.  \n","   - Each pair `(a[i], b[i])` is multiplied by retrieving its result from the LUT.\n","\n","2. **`manual_matrix_multiplication(a, b)`**  \n","   - Performs matrix multiplication manually using the above element-wise function.  \n","   - For each output position `(i, j)`, it multiplies the `i`-th row of `a` and the `j`-th column of `b` element-wise and sums the results.\n","\n","3. **`dense_layer(input_vector, weights, bias)`**  \n","   - Computes the layer output by multiplying the input vector with the weight matrix using the manual matrix multiplication function, then adds the bias term.\n","\n","---\n","\n","### ğŸ§  **Summary**\n","This dense layer replaces standard NumPy or TensorFlow multiplications with **LUT-based arithmetic**, ensuring that every multiply operation in the network is handled through precomputed LUT lookups â€” maintaining consistency with our manual CNN design.\n"]},{"cell_type":"code","execution_count":11,"id":"3329a302-c7e5-46cd-baec-ca1c7e23422e","metadata":{"id":"3329a302-c7e5-46cd-baec-ca1c7e23422e","executionInfo":{"status":"ok","timestamp":1761734763231,"user_tz":-60,"elapsed":10,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["def manual_elementwise_1D_multiplication(a, b):\n","    \"\"\"\n","    Custom element-wise multiplication function.\n","    Multiplies elements of a and b using pre-computed arrays based on type t.\n","    \"\"\"\n","    a = np.array(a)\n","    b = np.array(b)\n","    a_shape = np.shape(a)\n","    b = np.reshape(b, a_shape)\n","    result = np.zeros(a_shape)\n","\n","    # Perform element-wise multiplication for 1D array using LUT multiplication\n","    for i in range(a_shape[0]):\n","        result[i] = LUT[int(a[i]) + 128, int(b[i]) + 128]\n","\n","    return result\n","\n","###########################################################################################################\n","\n","def manual_matrix_multiplication(a, b):\n","    \"\"\"\n","    Custom matrix multiplication using custom_elementwise_multiplication function.\n","    \"\"\"\n","    a = np.array(a)\n","    b = np.array(b)\n","    a_shape = np.shape(a)\n","    b_shape = np.shape(b)\n","    result = np.zeros([a_shape[0], b_shape[1]])\n","\n","    # Perform matrix multiplication\n","    for i in range(a_shape[0]):\n","        for j in range(b_shape[1]):\n","            result[i, j] = np.sum(manual_elementwise_1D_multiplication(a[i, :], b[:, j]))\n","    return result\n","\n","#############################################################################################################\n","\n","def dense_layer(input_vector, weights, bias):\n","\n","    output = manual_matrix_multiplication(input_vector, weights) + bias\n","\n","    return output"]},{"cell_type":"markdown","id":"334f1d83-aab3-476d-b36b-4cf4749375f8","metadata":{"id":"334f1d83-aab3-476d-b36b-4cf4749375f8"},"source":["## ğŸ§© **Step 10: Implementing the Manual Forward Pass**\n","\n","In this step, we define a **manual forward pass** function that performs the entire inference process through our LUT-based CNN model.  \n","This function manually propagates one input image through all convolutional and dense layers, applying activation functions and quantization at each stage.\n","\n","---\n","\n","### âš™ï¸ **How It Works**\n","\n","1. **Input Selection**  \n","   - The function takes an `image_index` and retrieves the corresponding image (`input_image`) from the dataset.\n","\n","2. **Convolutional Layers**  \n","   - The image passes sequentially through **five convolutional layers**.  \n","   - Each layer performs:\n","     - Convolution using the **manual LUT-based convolution function**.  \n","     - **ReLU activation** (`np.maximum(0, x)`) to introduce non-linearity.  \n","     - **Dynamic normalization and quantization**, scaling outputs into the range **[-127, 127]** to maintain 8-bit precision.\n","\n","3. **Flattening**  \n","   - The final feature map is reshaped into a 1D vector (`[1, 2304]`) to prepare it for the dense layers.\n","\n","4. **Dense Layers**  \n","   - The flattened vector passes through **three dense (fully connected) layers**, each followed by:\n","     - **ReLU activation**  \n","     - **Normalization and quantization** to keep outputs within the 8-bit range.\n","\n","5. **Output Prediction**  \n","   - The last dense layer produces a 10-element output vector (one for each digit class).  \n","   - The function selects the index of the maximum value using `np.argmax(x)`, representing the predicted digit.\n","\n","---\n","\n","### ğŸ§  **Summary**\n","\n","This manual forward pass simulates a complete CNN inference pipeline using:\n","- **LUT-based multiplications** in all convolution and dense operations,  \n","- **Manual ReLU activations**,  \n","- **Dynamic quantization** after each layer.\n","\n","By doing so, it reproduces the behavior of a quantized CNN entirely through low-level operations â€” providing a clear, step-by-step view of how each layer transforms the input into a final classification.\n"]},{"cell_type":"code","execution_count":12,"id":"24377b86-9ca3-4729-96b4-56efc474232a","metadata":{"id":"24377b86-9ca3-4729-96b4-56efc474232a","executionInfo":{"status":"ok","timestamp":1761734766137,"user_tz":-60,"elapsed":44,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["def manual_forward_pass(image_index):\n","    \"\"\"\n","    Custom function to perform forward pass through the modified model.\n","    \"\"\"\n","    # Perform a series of convolutions and activations\n","    input_image = input_features[image_index]\n","\n","\n","    x = convolution_layer(input_image, model_weights[0], model_weights[1])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = convolution_layer(x, model_weights[2], model_weights[3])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = convolution_layer(x, model_weights[4], model_weights[5])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = convolution_layer(x, model_weights[6], model_weights[7])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = convolution_layer(x, model_weights[8], model_weights[9])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = np.reshape(x, [1, 2304])\n","\n","\n","    x = dense_layer(x, model_weights[10], model_weights[11])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = dense_layer(x, model_weights[12], model_weights[13])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = dense_layer(x, model_weights[14], model_weights[15])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","    detected_class = np.argmax(x)\n","\n","    return detected_class"]},{"cell_type":"markdown","id":"61d23b89-decc-40d5-afa4-ba667bbb10d2","metadata":{"id":"61d23b89-decc-40d5-afa4-ba667bbb10d2"},"source":["## ğŸ§© **Step 11: Evaluating Inference Performance and Accuracy**\n","\n","In this final step, we evaluate the **inference speed and accuracy** of our manually implemented CNN that uses LUT-based multiplications.\n","\n"]},{"cell_type":"code","execution_count":13,"id":"99492540-214c-435a-a046-5c83610a7792","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99492540-214c-435a-a046-5c83610a7792","executionInfo":{"status":"ok","timestamp":1761735007209,"user_tz":-60,"elapsed":238609,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}},"outputId":"072ae8ca-c6c6-4f94-e65f-fdc8b4f584e1"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:58<00:00, 23.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Execution time: 238.56162428855896 seconds\n","Accuracy with multiplier: 1.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# use 1000 test image from mnist dataset\n","batch_size = 1000\n","input_features = np.array(x_test[:batch_size], dtype=np.float32)\n","\n","start_time = time.time()\n","nb_imgs = 10\n","\n","results = []\n","for image_index in tqdm(range(nb_imgs)):\n","    results.append(int(manual_forward_pass(image_index)))\n","\n","elapsed_time = time.time() - start_time\n","print('Execution time:', elapsed_time, 'seconds')\n","\n","accuracy = np.sum(results == y_test[:len(results)]) / len(results)\n","print(f\"Accuracy with multiplier: {accuracy}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}