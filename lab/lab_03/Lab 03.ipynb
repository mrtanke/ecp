{"cells":[{"cell_type":"markdown","id":"6e96b120-4cca-4089-ac37-09661299e1b1","metadata":{"id":"6e96b120-4cca-4089-ac37-09661299e1b1"},"source":["#  **Lab 03: Implementing a Manual Quantized CNN with Look-Up Table (LUT)-Based Multiplication**\n","\n","## üîç **Introduction**\n","In this lab, we will explore how to implement a **manual Convolutional Neural Network (CNN)** from scratch and replace its built-in multiplication operations with a **Look-Up Table (LUT)-based multiplication** mechanism.  \n","\n","In typical CNN implementations, each convolution or dense layer performs thousands of multiplications between input activations and weights.  \n","When implementing **approximate multipliers**, repeatedly calling a custom function for each multiplication would be very slow.  \n","A much more efficient approach is to use a **Look-Up Table (LUT)** that stores the precomputed results of all possible multiplication pairs.  \n","Then, during inference, the model can simply read results from the table instead of performing direct arithmetic ‚Äî a concept that significantly reduces computation time.\n","\n","In this lab, to simplify the concept and keep the focus on the **LUT mechanism itself**, we will **use the LUT of an exact multiplier** (i.e., the results are mathematically correct).  \n","This allows you to understand how LUT-based computation works before moving to **approximate multipliers** in your homework.\n","\n","---\n","\n","## üéØ **Objectives**\n","By the end of this lab, you will be able to:\n","\n","1. **Implement** a manual 2D convolution function using LUT-based (exact) multiplication.  \n","2. **Modify** dense (fully connected) layers to use LUT-based multiplication as well.  \n","3. **Construct** a quantized *LeNet5-Inspired CNN* entirely from scratch using your custom convolution and dense functions.  \n","4. **Evaluate** the CNN‚Äôs performance and accuracy when all multiplications are replaced with LUT lookups.\n","\n","---\n","\n","> üß† **Key Concept:**  \n","> Instead of computing `a √ó b` directly, we use a **Look-Up Table** to fetch the result of that multiplication.  \n","> This approach forms the foundation for implementing **approximate arithmetic**, which you will explore in your homework.\n"]},{"cell_type":"markdown","id":"e1c68a71-ecdb-4b7f-bc79-2e85efa1f068","metadata":{"id":"e1c68a71-ecdb-4b7f-bc79-2e85efa1f068"},"source":["## üß© **Step 1: Import Required Libraries**"]},{"cell_type":"code","execution_count":1,"id":"516b079a-bd74-4a7e-b186-b25f8cf9beae","metadata":{"id":"516b079a-bd74-4a7e-b186-b25f8cf9beae","executionInfo":{"status":"ok","timestamp":1761732423115,"user_tz":-60,"elapsed":6024,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, Model\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","from tqdm import tqdm"]},{"cell_type":"markdown","id":"f45ad6a0-dc65-4d33-aa90-814d9d0ed06a","metadata":{"id":"f45ad6a0-dc65-4d33-aa90-814d9d0ed06a"},"source":["## üß© **Step 2: Load the MNIST Dataset**"]},{"cell_type":"code","execution_count":2,"id":"9e090b1e-2155-4e07-8293-bbc3738e8e2b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"9e090b1e-2155-4e07-8293-bbc3738e8e2b","executionInfo":{"status":"ok","timestamp":1761732423943,"user_tz":-60,"elapsed":823,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}},"outputId":"2c3e2026-462d-4783-a944-f9e583ea75e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x300 with 5 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training data shape: (60000, 28, 28, 1) (60000,)\n","Test data shape: (10000, 28, 28, 1) (10000,)\n"]}],"source":["# -------------------------\n","# 1) Data: load + preprocess\n","# -------------------------\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","# MNIST is 28x28 grayscale.\n","x_train = x_train.astype(\"uint8\")\n","x_test  = x_test.astype(\"uint8\")\n","x_train = x_train.reshape(-1, 28, 28, 1)\n","x_test  = x_test.reshape(-1, 28, 28, 1)\n","\n","x_test_fp = x_test\n","\n","#To visualize three test images from the testing set along with their corresponding labels, you can use the following code:\n","# Define the number of images to display\n","num_images = 5\n","\n","# Create subplots for each image\n","plt.figure(figsize=(10, 3))\n","for i in range(num_images):\n","    plt.subplot(1, num_images, i + 1)\n","    plt.imshow(x_test[i], cmap='gray')\n","    plt.title(f\"Label: {y_test[i]}\")\n","    plt.axis('off')  # Hide the axis\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Check the shape of the data\n","print(\"Training data shape:\", x_train.shape, y_train.shape)\n","print(\"Test data shape:\", x_test.shape, y_test.shape)\n"]},{"cell_type":"markdown","id":"efbd8612-d76e-43ab-9c6f-43465d6d907b","metadata":{"id":"efbd8612-d76e-43ab-9c6f-43465d6d907b"},"source":["## üß© **Step 3: Applying 8-bit Quantization to the MNIST Dataset**"]},{"cell_type":"code","execution_count":3,"id":"2939d850-271a-4fc6-ba02-e1bea1eeebd2","metadata":{"id":"2939d850-271a-4fc6-ba02-e1bea1eeebd2","executionInfo":{"status":"ok","timestamp":1761732424410,"user_tz":-60,"elapsed":464,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["# 1. Load MNIST dataset\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","# 2Ô∏è. Convert to float32 for safe division\n","x_test = tf.cast(x_test, tf.float32)\n","\n","# 3Ô∏è. Normalize pixel values from [0, 255] ‚Üí [0, 1]\n","x_test = x_test / 255.0\n","\n","# 4Ô∏è. Scale normalized values from [0, 1] ‚Üí [0, 127]\n","x_test = x_test * 127.0\n","\n","# 5Ô∏è. Round to nearest integer\n","x_test = tf.round(x_test)\n","\n","# 6Ô∏è. Clip to make sure everything is within [0, 127]\n","x_test = tf.clip_by_value(x_test, 0.0, 127.0)\n","\n","# 7. Reshape to (num_samples, 28, 28, 1)\n","x_test = tf.reshape(x_test, (-1, 28, 28, 1))\n"]},{"cell_type":"markdown","id":"a0d6b21a-7164-4b34-9fad-fdcc4a050404","metadata":{"id":"a0d6b21a-7164-4b34-9fad-fdcc4a050404"},"source":["## üß© **Step 4: Load the Pre-trained LeNet-5 Inspired Model with Quantization**\n","\n","In this step, we will load a **pre-trained and quantized LeNet-5-Inspired model**.  \n","This model has already been trained on the MNIST dataset to recognize handwritten digits and is designed to operate using **quantized weights and activations**.  \n","By using this pre-trained network, we can focus on understanding **how inference works** and how to integrate **LUT-based arithmetic**.\n","\n","---\n","\n","### üß† **What is Quantization?**\n","\n","Quantization is the process of converting the **32-bit floating-point** values used in neural networks (for weights, biases, and activations) into **lower-precision integer values** ‚Äî often 8-bit integers. This helps to make the network smaller, faster, and more efficient for deployment on edge devices or hardware accelerators.\n","\n","---\n","\n","### ‚öñÔ∏è **Advantages and Trade-offs of Quantization**\n","\n","#### ‚úÖ *Advantages:*\n","- **Reduced Model Size:** Lower-bit weights take up less memory, which is crucial for embedded devices.  \n","- **Faster Inference:** Integer arithmetic is much faster than floating-point arithmetic, enabling near real-time performance.  \n","- **Lower Power Usage:** Fewer computational resources mean reduced power consumption ‚Äî ideal for battery-powered systems.\n","\n","#### ‚ö†Ô∏è *Potential Drawbacks:*\n","- **Slight Accuracy Loss:** Converting from float to int can introduce rounding errors.  \n","- **Limited Precision:** Very small weight updates or activation variations may be lost due to the reduced numerical range.\n","\n","---\n","> We will later apply our **LUT-based multiplications** to this quantized model to simulate integer arithmetic behavior.\n"]},{"cell_type":"code","execution_count":4,"id":"9d6b9efa-aca2-434c-b543-3ebc270a6af8","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":848},"id":"9d6b9efa-aca2-434c-b543-3ebc270a6af8","executionInfo":{"status":"ok","timestamp":1761732424793,"user_tz":-60,"elapsed":378,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}},"outputId":"0ffff921-5337-4a7a-e00e-9eb3974b6896"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n","‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n","‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n","‚îÇ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     ‚îÇ           \u001b[38;5;34m128\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_8 (\u001b[38;5;33mActivation\u001b[0m)       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     ‚îÇ         \u001b[38;5;34m2,080\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_9 (\u001b[38;5;33mActivation\u001b[0m)       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     ‚îÇ           \u001b[38;5;34m528\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_10 (\u001b[38;5;33mActivation\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m8\u001b[0m)      ‚îÇ         \u001b[38;5;34m1,160\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_11 (\u001b[38;5;33mActivation\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m8\u001b[0m)      ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m8\u001b[0m)      ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m4\u001b[0m)      ‚îÇ           \u001b[38;5;34m292\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_12 (\u001b[38;5;33mActivation\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m4\u001b[0m)      ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m4\u001b[0m)      ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ       \u001b[38;5;34m295,040\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_13 (\u001b[38;5;33mActivation\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ         \u001b[38;5;34m8,256\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_14 (\u001b[38;5;33mActivation\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ           \u001b[38;5;34m650\u001b[0m ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_15 (\u001b[38;5;33mActivation\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n","‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n","‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n","‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n","‚îÇ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,160</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> ‚îÇ\n","‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n","‚îÇ activation_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n","‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m308,134\u001b[0m (1.18 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">308,134</span> (1.18 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m308,134\u001b[0m (1.18 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">308,134</span> (1.18 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["# Load your trained FP32 model\n","quantized_model = load_model(\"my_org_model_top4_quant.h5\", compile=False)\n","quantized_model.summary()"]},{"cell_type":"markdown","id":"58e24ba5-6a76-47ab-97d7-7452521d8209","metadata":{"id":"58e24ba5-6a76-47ab-97d7-7452521d8209"},"source":["## üß© **Step 5: Overview and Analysis of Our CNN Model Architecture**\n","\n","In this step, we will explore the structure of our **custom CNN model**, which is inspired by the classic **LeNet-5 architecture** ‚Äî one of the first successful convolutional neural networks for image classification. This model is designed to recognize handwritten digits (0‚Äì9) from the MNIST dataset.  \n","We will also retrieve and examine the **weights** and **biases** of each layer to better understand how the model processes input data and learns visual patterns.\n","\n","---\n","\n","### üèóÔ∏è **Model Structure**\n","\n","Our CNN consists of three main components:\n","\n","#### üîπ **Convolutional Layers**\n","- The model begins with **five convolutional layers**.  \n","  Each layer applies a set of **filters (kernels)** that slide across the input image to detect specific local patterns such as edges, corners, and textures.  \n","- After each convolution, an **activation function (ReLU)** introduces non-linearity, allowing the network to learn more complex representations of the data.\n","\n","#### üîπ **Flattening Layer**\n","- Once the convolutional feature extraction is complete, the resulting 2D feature maps are **flattened** into a 1D array.  \n","  This operation prepares the extracted features for the **fully connected layers**, which handle the final classification.\n","\n","#### üîπ **Dense (Fully Connected) Layers**\n","- After flattening, the features are passed through **three dense layers**.  \n","  The first two dense layers are followed by activation functions that help the network learn higher-level feature combinations.  \n","  The final dense layer outputs **10 values**, each representing the probability that the input image belongs to one of the 10 digit classes (0‚Äì9).\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":5,"id":"1fbafff4-dcc8-4786-ba48-06d0ee7f07ca","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1fbafff4-dcc8-4786-ba48-06d0ee7f07ca","executionInfo":{"status":"ok","timestamp":1761732424796,"user_tz":-60,"elapsed":2,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}},"outputId":"4a0b6643-96ff-457a-e4ca-3f742064c4c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------\n","Convolution Layers:\n","----------------------------------------\n","Layer: Conv2D\n","  Filters: shape = (1, 1, 1, 64)\n","  Biases: shape = (64,)\n","Layer: Conv2D\n","  Filters: shape = (1, 1, 64, 32)\n","  Biases: shape = (32,)\n","Layer: Conv2D\n","  Filters: shape = (1, 1, 32, 16)\n","  Biases: shape = (16,)\n","Layer: Conv2D\n","  Filters: shape = (3, 3, 16, 8)\n","  Biases: shape = (8,)\n","Layer: Conv2D\n","  Filters: shape = (3, 3, 8, 4)\n","  Biases: shape = (4,)\n","----------------------------------------\n","Dense Layers (fully-connected Layers):\n","----------------------------------------\n","Dense_Layer 1 has 128 neurons\n","Dense_Layer 2 has 64 neurons\n","Dense_Layer 3 has 10 neurons\n"]}],"source":["# Get model weights and print model summary\n","model_weights = quantized_model.get_weights()\n","\n","print(\"-\"*40)\n","print(\"Convolution Layers:\")\n","print(\"-\"*40)\n","for layer in quantized_model.layers:\n","    if 'conv' in layer.name:  # This filters only convolutional layers\n","        print(f\"Layer: Conv2D\")\n","\n","        # Get the weights and biases of the layer\n","        filters, biases = layer.get_weights()\n","\n","        print(f\"  Filters: shape = {filters.shape}\")\n","        print(f\"  Biases: shape = {biases.shape}\")\n","i =0\n","print(\"-\"*40)\n","print(\"Dense Layers (fully-connected Layers):\")\n","print(\"-\"*40)\n","for layer in quantized_model.layers:\n","    if 'dense' in layer.name:  # This filters only dense layers\n","        i += 1\n","        print(f\"Dense_Layer {i} has {layer.units} neurons\")"]},{"cell_type":"markdown","id":"6b38969d-eace-4bd5-8d35-c59dc2f74783","metadata":{"id":"6b38969d-eace-4bd5-8d35-c59dc2f74783"},"source":["## üß© **Step 6: Creating a Look-Up Table (LUT) for Multiplication**\n","\n","In this step, we will construct a **Look-Up Table (LUT)** that replaces traditional multiplication operations with **table lookups**.  \n","Instead of performing arithmetic calculations during convolution or dense operations, our model will **retrieve precomputed multiplication results** from the LUT.\n","\n","Since our CNN uses **8-bit signed integers** for weights and activations, the LUT must cover all possible combinations of 8-bit signed values ‚Äî from **-128 to +127**.\n","\n","---\n","\n","### üîπ **Part 1: Building the LUT for Signed 8-Bit Multiplication**\n","\n","#### ‚öôÔ∏è **LUT Range**\n","The LUT needs to store results for every possible pair of signed 8-bit integers.\n","\n","This creates a **256 √ó 256 table**, where each entry at position `(i, j)` represents the product of two integers `a √ó b`.\n","\n","#### üíæ **Storage Concept**\n","- Each LUT entry corresponds to a **unique pair** of integers.  \n","- Once built, the LUT allows fast access ‚Äî for example:\n","  ```python\n","`a √ó b` = `LUT[a + 128][b + 128]`\n","\n","#### Why use `+128` for LUT indexing?\n","The `+128` offset ensures that signed integers in the range **[-128, 127]** map to valid LUT indices **[0, 255]**.  \n","Example: `-128 ‚Üí 0`, `0 ‚Üí 128`, `127 ‚Üí 255`.\n","\n","---\n","\n","### üîπ **Part 2: Implementing the Manual Element-Wise Multiplication Function**\n","\n","Next, we‚Äôll define a **custom element-wise multiplication function** that uses our LUT.  \n","This function will handle **2-dimensional element-wise multiplication** of two matrices with the same size ‚Äî a common operation in **2D convolution** layers.\n"]},{"cell_type":"code","execution_count":6,"id":"7a28e3f8-5731-44b5-9aeb-41d073453382","metadata":{"id":"7a28e3f8-5731-44b5-9aeb-41d073453382","executionInfo":{"status":"ok","timestamp":1761732424807,"user_tz":-60,"elapsed":1,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["LUT = np.zeros((256,256),dtype=np.int32)\n","for i in range(-128,128):\n","    for j in range(-128,128):\n","        LUT[i +128,j+128]= i * j"]},{"cell_type":"code","execution_count":7,"id":"1543616c-8085-4daf-8f28-51838303911d","metadata":{"id":"1543616c-8085-4daf-8f28-51838303911d","executionInfo":{"status":"ok","timestamp":1761732424809,"user_tz":-60,"elapsed":1,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["def manual_elementwise_multiplication(a, b):\n","    \"\"\"\n","    Custom element-wise multiplication function.\n","    Multiplies elements of a and b using pre-computed arrays based on type t.\n","    \"\"\"\n","    a = np.array(a)\n","    b = np.array(b)\n","    a_shape = np.shape(a)\n","    b = np.reshape(b, a_shape)\n","    result = np.zeros(a_shape)\n","\n","    for i in range(a_shape[0]):\n","        for j in range(a_shape[1]):\n","            result[i, j] = LUT[int(a[i, j]) + 128, int(b[i, j]) + 128]\n","\n","    return result"]},{"cell_type":"markdown","id":"93de073c-8b78-494c-be1e-e1f3f20dc6e9","metadata":{"id":"93de073c-8b78-494c-be1e-e1f3f20dc6e9"},"source":["## üß© **Step 7: Implementing a Manual 1-Channel 2D Convolution Function with LUT-Based Multiplication**\n","\n","In this step, we implement a **custom 1-channel 2D convolution function** that uses the LUT for all multiplications.  \n","This function performs element-wise multiplications using the LUT instead of standard arithmetic.\n","\n","---\n","\n","### ‚öôÔ∏è **Convolution Overview**\n","\n","A **2D convolution** slides a filter (kernel) across the input image.  \n","At each position, it multiplies the kernel elements with the corresponding input patch and sums the results to form one output value, representing a detected feature.\n","\n","---\n","\n","### üß† **Key Components**\n","\n","- **Sliding Window:**  \n","  The function extracts a patch of the input that matches the filter size for each position.\n","\n","- **LUT-Based Multiplication:**  \n","  Each element-wise multiplication between the input patch and the filter is replaced by a **lookup** from the LUT.\n","\n","- **Summation:**  \n","  All lookup results are summed to produce the final value for that output position in the feature map.\n"]},{"cell_type":"code","execution_count":8,"id":"1b9563cc-670d-4c26-a778-2b81ee491245","metadata":{"id":"1b9563cc-670d-4c26-a778-2b81ee491245","executionInfo":{"status":"ok","timestamp":1761732424810,"user_tz":-60,"elapsed":1,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["def manual_convolution(image, kernel):\n","    \"\"\"\n","    manual 2D convolution using manual_elementwise_multiplication function.\n","    \"\"\"\n","    # Determine the dimensions of image and kernel\n","    image_height, image_width = image.shape\n","    kernel_height, kernel_width = kernel.shape\n","\n","    # Determine the output dimensions based on the dimensions of the input image and kernel\n","    output_height = image_height - kernel_height + 1\n","    output_width = image_width - kernel_width + 1\n","\n","    # Initialize the output image by zero\n","    output = np.zeros((output_height, output_width))\n","\n","    # Perform convolution\n","    for i in range(output_height):\n","        for j in range(output_width):\n","            # Element-wise multiplication of the kernel and the corresponding image region\n","            region = image[i:i+kernel_height, j:j+kernel_width]\n","            output[i, j] = np.sum(manual_elementwise_multiplication(region , kernel))\n","\n","    return output"]},{"cell_type":"markdown","id":"981f4bf5-c821-4865-99d0-49a21edcf5a4","metadata":{"id":"981f4bf5-c821-4865-99d0-49a21edcf5a4"},"source":["## üß© **Step 8: Implementing the Complete 2D Convolution Operation**\n","\n","Now that we have built the LUT-based 1-channel convolution function,  \n","we can extend it to perform a **full 2D convolution** that supports multiple input channels and multiple filters (kernels), just like real CNN layers.\n","\n","---\n","\n","### ‚öôÔ∏è **Understanding Full 2D Convolution**\n","\n","In a CNN, the convolution operation is not limited to a single channel or a single kernel.  \n","Instead, we can have:\n","- **Multiple input channels** (e.g., from color channels or previous feature maps).\n","- **Multiple kernels (filters)**, where each kernel produces one output feature map.\n","\n","Each kernel contains a separate **2D filter for each input channel**.  \n","During convolution, the process works as follows:\n","\n","1. For a given kernel:\n","   - Each input channel is convolved with its corresponding kernel slice using the **1-channel LUT-based convolution**.\n","   - The results from all channels are **summed** element-wise to produce one **output feature map**.\n","\n","2. This process is **repeated for all kernels**, meaning:\n","   - Each kernel produces one output feature map.\n","   - If there are multiple kernels, their results are **stacked** along the channel dimension, forming the final multi-channel output.\n","\n","---\n","\n","### üîÅ **Summary of the Process**\n","1. Loop over each **kernel** (filter).  \n","2. For each kernel:\n","   - Loop over all **input channels**.  \n","   - Perform the **LUT-based convolution** between the input channel and the corresponding kernel slice.  \n","   - **Sum** all the resulting feature maps across channels.  \n","3. Stack all kernel outputs to form the final multi-channel output.\n","\n","---\n","\n","> üí° **Key Idea:**  \n","> A full 2D convolution is essentially a combination of many **1-channel convolutions**,  \n","> where results are **summed across channels** and **repeated across kernels** to extract rich features from the input.\n"]},{"cell_type":"code","execution_count":10,"id":"926afed0-b3f2-4aa3-b6d1-fecb58edc3b4","metadata":{"id":"926afed0-b3f2-4aa3-b6d1-fecb58edc3b4","executionInfo":{"status":"ok","timestamp":1761734608228,"user_tz":-60,"elapsed":2,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["def convolution_layer(input_feature_map, kernel, bias):\n","\n","    input_shape = input_feature_map.shape # [input_height, output_height, channels]\n","    kernel_shape = kernel.shape # [kernel_height, kernel_weight, in_channels, out_channels]\n","\n","    output_height = input_shape[0] - kernel_shape[0] + 1\n","    output_width = input_shape[1] - kernel_shape[1] + 1\n","    output_channels = kernel_shape[3]\n","\n","    output = np.zeros([output_height, output_width, output_channels])\n","\n","    for i in range(output_channels):\n","        for j in range(kernel_shape[2]):\n","            output[:, :, i] = output[:, :, i] + manual_convolution( input_feature_map[ :, :,j] , kernel[ :, :, j, i] )\n","        output[:, :, i] = output[:, :, i] + bias[i]\n","\n","    return output"]},{"cell_type":"markdown","id":"42e3b9ec-2bb6-4c88-976f-ee61328eb58c","metadata":{"id":"42e3b9ec-2bb6-4c88-976f-ee61328eb58c"},"source":["## üß© **Step 9: Implementing a Manual Dense (Fully Connected) Layer**\n","\n","In this step, we implement a **manual dense layer** that performs matrix multiplication using our **LUT-based multiplication** functions.\n","\n","---\n","\n","### ‚öôÔ∏è **How It Works**\n","\n","1. **`manual_elementwise_1D_multiplication(a, b)`**  \n","   - Performs element-wise multiplication between two 1D vectors using the **LUT** instead of the `*` operator.  \n","   - Each pair `(a[i], b[i])` is multiplied by retrieving its result from the LUT.\n","\n","2. **`manual_matrix_multiplication(a, b)`**  \n","   - Performs matrix multiplication manually using the above element-wise function.  \n","   - For each output position `(i, j)`, it multiplies the `i`-th row of `a` and the `j`-th column of `b` element-wise and sums the results.\n","\n","3. **`dense_layer(input_vector, weights, bias)`**  \n","   - Computes the layer output by multiplying the input vector with the weight matrix using the manual matrix multiplication function, then adds the bias term.\n","\n","---\n","\n","### üß† **Summary**\n","This dense layer replaces standard NumPy or TensorFlow multiplications with **LUT-based arithmetic**, ensuring that every multiply operation in the network is handled through precomputed LUT lookups ‚Äî maintaining consistency with our manual CNN design.\n"]},{"cell_type":"code","execution_count":11,"id":"3329a302-c7e5-46cd-baec-ca1c7e23422e","metadata":{"id":"3329a302-c7e5-46cd-baec-ca1c7e23422e","executionInfo":{"status":"ok","timestamp":1761734763231,"user_tz":-60,"elapsed":10,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["def manual_elementwise_1D_multiplication(a, b):\n","    \"\"\"\n","    Custom element-wise multiplication function.\n","    Multiplies elements of a and b using pre-computed arrays based on type t.\n","    \"\"\"\n","    a = np.array(a)\n","    b = np.array(b)\n","    a_shape = np.shape(a)\n","    b = np.reshape(b, a_shape)\n","    result = np.zeros(a_shape)\n","\n","    # Perform element-wise multiplication for 1D array using LUT multiplication\n","    for i in range(a_shape[0]):\n","        result[i] = LUT[int(a[i]) + 128, int(b[i]) + 128]\n","\n","    return result\n","\n","###########################################################################################################\n","\n","def manual_matrix_multiplication(a, b):\n","    \"\"\"\n","    Custom matrix multiplication using custom_elementwise_multiplication function.\n","    \"\"\"\n","    a = np.array(a)\n","    b = np.array(b)\n","    a_shape = np.shape(a)\n","    b_shape = np.shape(b)\n","    result = np.zeros([a_shape[0], b_shape[1]])\n","\n","    # Perform matrix multiplication\n","    for i in range(a_shape[0]):\n","        for j in range(b_shape[1]):\n","            result[i, j] = np.sum(manual_elementwise_1D_multiplication(a[i, :], b[:, j]))\n","    return result\n","\n","#############################################################################################################\n","\n","def dense_layer(input_vector, weights, bias):\n","\n","    output = manual_matrix_multiplication(input_vector, weights) + bias\n","\n","    return output"]},{"cell_type":"markdown","id":"334f1d83-aab3-476d-b36b-4cf4749375f8","metadata":{"id":"334f1d83-aab3-476d-b36b-4cf4749375f8"},"source":["## üß© **Step 10: Implementing the Manual Forward Pass**\n","\n","In this step, we define a **manual forward pass** function that performs the entire inference process through our LUT-based CNN model.  \n","This function manually propagates one input image through all convolutional and dense layers, applying activation functions and quantization at each stage.\n","\n","---\n","\n","### ‚öôÔ∏è **How It Works**\n","\n","1. **Input Selection**  \n","   - The function takes an `image_index` and retrieves the corresponding image (`input_image`) from the dataset.\n","\n","2. **Convolutional Layers**  \n","   - The image passes sequentially through **five convolutional layers**.  \n","   - Each layer performs:\n","     - Convolution using the **manual LUT-based convolution function**.  \n","     - **ReLU activation** (`np.maximum(0, x)`) to introduce non-linearity.  \n","     - **Dynamic normalization and quantization**, scaling outputs into the range **[-127, 127]** to maintain 8-bit precision.\n","\n","3. **Flattening**  \n","   - The final feature map is reshaped into a 1D vector (`[1, 2304]`) to prepare it for the dense layers.\n","\n","4. **Dense Layers**  \n","   - The flattened vector passes through **three dense (fully connected) layers**, each followed by:\n","     - **ReLU activation**  \n","     - **Normalization and quantization** to keep outputs within the 8-bit range.\n","\n","5. **Output Prediction**  \n","   - The last dense layer produces a 10-element output vector (one for each digit class).  \n","   - The function selects the index of the maximum value using `np.argmax(x)`, representing the predicted digit.\n","\n","---\n","\n","### üß† **Summary**\n","\n","This manual forward pass simulates a complete CNN inference pipeline using:\n","- **LUT-based multiplications** in all convolution and dense operations,  \n","- **Manual ReLU activations**,  \n","- **Dynamic quantization** after each layer.\n","\n","By doing so, it reproduces the behavior of a quantized CNN entirely through low-level operations ‚Äî providing a clear, step-by-step view of how each layer transforms the input into a final classification.\n"]},{"cell_type":"code","execution_count":12,"id":"24377b86-9ca3-4729-96b4-56efc474232a","metadata":{"id":"24377b86-9ca3-4729-96b4-56efc474232a","executionInfo":{"status":"ok","timestamp":1761734766137,"user_tz":-60,"elapsed":44,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}}},"outputs":[],"source":["def manual_forward_pass(image_index):\n","    \"\"\"\n","    Custom function to perform forward pass through the modified model.\n","    \"\"\"\n","    # Perform a series of convolutions and activations\n","    input_image = input_features[image_index]\n","\n","\n","    x = convolution_layer(input_image, model_weights[0], model_weights[1])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = convolution_layer(x, model_weights[2], model_weights[3])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = convolution_layer(x, model_weights[4], model_weights[5])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = convolution_layer(x, model_weights[6], model_weights[7])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = convolution_layer(x, model_weights[8], model_weights[9])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = np.reshape(x, [1, 2304])\n","\n","\n","    x = dense_layer(x, model_weights[10], model_weights[11])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = dense_layer(x, model_weights[12], model_weights[13])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","\n","    x = dense_layer(x, model_weights[14], model_weights[15])\n","    x = np.maximum(0, x) # RelU\n","    x = x / np.max(np.abs(x))\n","    x = np.round(x * 127.0)\n","    x = np.clip(x, -127.0, 127.0)\n","\n","    detected_class = np.argmax(x)\n","\n","    return detected_class"]},{"cell_type":"markdown","id":"61d23b89-decc-40d5-afa4-ba667bbb10d2","metadata":{"id":"61d23b89-decc-40d5-afa4-ba667bbb10d2"},"source":["## üß© **Step 11: Evaluating Inference Performance and Accuracy**\n","\n","In this final step, we evaluate the **inference speed and accuracy** of our manually implemented CNN that uses LUT-based multiplications.\n","\n"]},{"cell_type":"code","execution_count":13,"id":"99492540-214c-435a-a046-5c83610a7792","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99492540-214c-435a-a046-5c83610a7792","executionInfo":{"status":"ok","timestamp":1761735007209,"user_tz":-60,"elapsed":238609,"user":{"displayName":"stevence tan","userId":"15336308596270623544"}},"outputId":"072ae8ca-c6c6-4f94-e65f-fdc8b4f584e1"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [03:58<00:00, 23.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Execution time: 238.56162428855896 seconds\n","Accuracy with multiplier: 1.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# use 1000 test image from mnist dataset\n","batch_size = 1000\n","input_features = np.array(x_test[:batch_size], dtype=np.float32)\n","\n","start_time = time.time()\n","nb_imgs = 10\n","\n","results = []\n","for image_index in tqdm(range(nb_imgs)):\n","    results.append(int(manual_forward_pass(image_index)))\n","\n","elapsed_time = time.time() - start_time\n","print('Execution time:', elapsed_time, 'seconds')\n","\n","accuracy = np.sum(results == y_test[:len(results)]) / len(results)\n","print(f\"Accuracy with multiplier: {accuracy}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}